{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning a Model for Transliteration of Polish Names and Toponyms\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](./transliteration.ipynb)\n",
    "---\n",
    "## Introduction\n",
    "Transliteration is a common task in natural language processing (NLP) and machine translation. Here, we will use Polish, a Slavic language. Many words came from the Proto-Slavic language, an ancestor to all Slavic languages. In most languages (Serbian, Bulgarian, etc.), these words are scribed in Cyrillic; however, speakers in Polish and some other languages use the Latin script. This creates unusual writings: for one, the word *świekier* (English: *father-in-law*) came from the word `*svekrъ`, which is also a common ancestor for the Russian *свёкр* (pronounced as *svjokr*), Serbian *свѐкар* (pronounced as *svekar*), and Slovak *svokor*.\n",
    "\n",
    "The same is true for names and toponyms. For example, there is a Polish name, *Bogumił*. You may think it can simply be transliterated to English as *Bogomil* — not at all! Polish *Bogumił* consists of two old Slavic roots: *Bog* (God) and *mił* (beloved, adored). The name means to be loved by God and is translated to English as *Theodore*. *Theodore* is a name that has Ancient Greek roots and means *gift of Gods*.\n",
    "\n",
    "The original Ancient Greek name *Θεόδωρος* (Theódoros) had two types of inheritance into the modern European languages:\n",
    "\n",
    "- By transliteration: Spanish *Theodoro*, French *Théodore*, Italian *Teodoro*, and Serbian *Теодор* (*Teodor*)\n",
    "- By translation: Spanish *Diosdado*, French *Dieudonné*, Italian *Donato*, and Serbian *Божидар* (*Božidar*)\n",
    "\n",
    "All European languages except English have two sibling names (transliterated and translated). English has only one form — Theodore. Thus, the translation of Polish Bogumił is a translation with a slight semantic change: instead of beloved by God, it means a gift of Gods. This and many other exciting nuances make Polish a perfect language to fine-tune a transliteration model on."
   ],
   "id": "6ab1c6746db4b036"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Table of Contents\n",
    "1. [Installation](#installation)\n",
    "2. [Preparing the Dataset](#preparing-a-corpus)\n",
    "3. [Tokenization](#tokenization)\n",
    "4. [Fine-tuning the Model](#fine-tuning)\n",
    "5. [Transliteration](#transliteration)"
   ],
   "id": "705658ab3fc56bca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Installation\n",
    "Before we begin, we need to install the necessary libraries."
   ],
   "id": "52f928cca13f8eef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install sentencepiece\n",
    "!pip install pandas\n",
    "!pip install lxml\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install huggingface_hub\n",
    "!pip install torch\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install accelerate\n",
    "!pip install wandb # for logging and monitoring the training process"
   ],
   "id": "91df1811cda8203c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preparing a corpus\n",
    "As you know, fine-tuning lets you adapt a pre-trained LLM to your specific domain or task, thus improving relevance, reducing inference cost, and injecting proprietary knowledge. We'll start by finding a proper dataset for fine-tuning. We'll create our own Polish names' transliteration corpus independently. For this, we will use open sources:\n",
    "\n",
    "- A Wikipedia page: [Appendix:Polish_given_names](https://en.wiktionary.org/wiki/Appendix:Polish_given_names). It has both feminine and masculine names.\n",
    "- A PDF file with multiple tables: [Toponymic Guidelines of Poland](https://www.gov.pl/web/ksng-en/toponymic-guidelines-of-poland). You will need only one for the country names table located on pages 45-51.\n",
    "\n",
    "You can download the files for the datasets from here:\n",
    "- [polish_names.csv](./datasets/polish_names.csv)\n",
    "- [toponyms_of_poland.csv](./datasets/toponyms_of_poland.csv).\n",
    "\n",
    "Next, you will need to combine them into one dataset, not forgetting to post-process it too:\n",
    "\n",
    "- When an explanation of the meaning is given instead of the equivalent in the `English` column, replace the value with `None`. You can easily locate these cases. They will include the `=` sign, for instance, `dob = + sław = 'fame, glory, renown'`.\n",
    "- Each cell must contain just one name. If there are two or more in the `English` column, select the first one. For example, if we have two equivalent names *(Janet, Jeanette)*, leave *Janet*.\n",
    "- There should be no extra symbols in the cell. Some country names have the following form: *Zjednoczone Emiraty\\rArabskie*. You should substitute \\r with a single space symbol, so the cell will have only *Zjednoczone Emiraty Arabskie*.\n",
    "- Don't forget to create a new, default index for the combined DataFrame (via *ignore_index* or *reset_index*).\n",
    "\n",
    "This is how your dataset should look like at this point:\n",
    "```csv\n",
    "                                Polish                 English\n",
    "0                                 Adam                    Adam\n",
    "1                               Adrian                  Adrian\n",
    "2                               Albert                  Albert\n",
    "3                                Albin                   Albin\n",
    "4                           Aleksander               Alexander\n",
    "..                                 ...                     ...\n",
    "631                     Wyspy Salomona         Solomon Islands\n",
    "632  Wyspy Świętego Tomasza i Książęca  S~ao Tomé and Príncipe\n",
    "633                             Zambia                  Zambia\n",
    "634                           Zimbabwe                Zimbabwe\n",
    "635       Zjednoczone Emiraty Arabskie    United Arab Emirates\n",
    "\n",
    "[636 rows x 2 columns]\n",
    "```\n",
    "We have a relatively small dataset which may not be enough for fine-tuning. To get good results, we'd need a much larger dataset. However, for educational purposes, we will use this small dataset to demonstrate the fine-tuning process."
   ],
   "id": "dbbbace4b29aeda7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# write your code here\n",
   "id": "edc5ce8d24a6b085"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenization\n",
    "\n",
    "Once you've created the dataset, the next step is transforming it into the Hugging Face dataset format. But first, split it into *train* and *test* sets with `sklearn`. The test set should be of size `0.078`. Also, make sure that `random_state` during the split is equal to 42.\n",
    "\n",
    "Next, transform the two sets into a Hugging Face dataset. You can use the following method:\n",
    "\n",
    "```python\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "\n",
    "dataset = DatasetDict({'train': Dataset.from_pandas(train),\n",
    "                       'test': Dataset.from_pandas(test)})\n",
    "\n",
    "```\n",
    "After this, you may push the dataset to your personal Hugging Face Hub. Now, let's turn to tokenization, the most challenging part of the Transformers' fine-tuning. During fine-tuning, as well as tokenization, you will use the [T5-base](https://huggingface.co/google-t5/t5-base)(`t5-base`) model. While loading it, you may need to set the following parameter: `model_max_length=512`.\n",
    "\n",
    "Next, we need to define a custom `preprocess_function()` for tokenization. Here is an example:\n",
    "\n",
    "```python\n",
    "max_input_length = 5  # Some toponyms contain 2-5 tokens\n",
    "max_target_length = 5\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    model_inputs = tokenizer(data['Polish'], max_length=max_input_length, truncation=True, padding=True)\n",
    "    labels = tokenizer(data['English'], max_length=max_target_length, truncation=True, padding=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "```\n",
    "At this point, ensure that `data['Polish']` and `data['English']` contain only string values. The `None` values should be represented as strings too.\n",
    "\n",
    "Finally, map the defined function both to the train and test sets and print their structure. While mapping, use `batched=True`.\n",
    "\n",
    "Here is an example of the structure of the train and test sets that you should get at the end of this stage (figures may vary):\n",
    "\n",
    "```python\n",
    "Dataset({\n",
    "    features: ['Polish', 'English', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
    "    num_rows: 600\n",
    "})\n",
    "Dataset({\n",
    "    features: ['Polish', 'English', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
    "    num_rows: 35\n",
    "})\n",
    "```"
   ],
   "id": "d9921a6ecb3eedad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# write your code here\n",
   "id": "62ca43366083e48e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine tuning\n",
    "\n",
    "Now, you will finally fine-tune your model! First, load the model itself with the `AutoModelForSeq2SeqLM` class. Apart from the training set, you need to define the following classes: `Seq2SeqTrainingArguments`, `DataCollatorForSeq2Seq`, and `Seq2SeqTrainer`.\n",
    "\n",
    "The first and default argument in `Seq2SeqTrainingArguments` is the name of your future model. Then, it's necessary to define the following arguments:\n",
    "- `eval_strategy` — *epoch*  # Evaluate the model after each epoch\n",
    "- `learning_rate` — *3e-6*  # Learning rate for the optimizer\n",
    "- `num_train_epochs` — 3 # Total number of training epochs (you can modify this as needed)\n",
    "- `weight_decay` — *0.01* # Amount of L2 regularization applied to the model's weights\n",
    "- `predict_with_generate=True` # Use generate to calculate evaluation loss and metrics\n",
    "- `fp16=True` # Use 16-bit floating point precision for training\n",
    "\n",
    "> Fine-tuning can take a lot of time and compute resources. To achieve good results with the model we use here and the relatively small dataset, we need to train for at least 50 epochs. For educational purposes, in this project, you can try with around 3-5 epochs (you can use the T4 GPU option for faster training — ~10 minutes).\n",
    "\n",
    "You will also need to compute the ROUGE score while training. For this, define two Python functions: `postprocess_text()` and `compute_metrics()`. In `compute_metrics()`, you need to extract the following metrics: ROUGE1, ROUGE2, ROUGEL, ROUGELSUM, and GEN_LEN (the mean length of the text generated on the validation set). These metrics will help you evaluate the quality of the model's output during training.\n",
    "\n",
    "The first part of the code above is just post-processing. At the start, we have only embeddings, neither words nor sentences. Transform them into normal words. The most important thing is the `compute_metrics()` function; it allows us to apply the ROUGE metric (or any other metric) to our validation set.\n",
    "\n",
    "As an additional step, use [Weights and Biases](https://wandb.ai/site) (`wandb`) for monitoring. All you need to do is create an account there, get the API key, and set the key and project in your notebook:\n",
    "```python\n",
    "from google.colab import userdata\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY') # set in secrets\n",
    "os.environ[\"WANDB_PROJECT\"] = \"my-project\" # use any name\n",
    "```\n",
    "Then, in your training arguments, set `report_to=wandb`. It will then track:\n",
    "- training/validation loss\n",
    "- learning rate\n",
    "- system metrics (GPU usage, RAM, etc.)\n",
    "\n",
    "And now, define the `Seq2SeqTrainer` and start training. You can save your model locally using the `save_model()` function. Alternatively, set the `output_dir` parameter in `Seq2SeqTrainingArguments`. This will save the model checkpoints to the specified directory and the model after fine-tuning is complete. You'll be able to use this model later for inference or further fine-tuning without the need to re-download it.\n",
    "\n",
    "To avoid losing the files, you can push the model to Hugging Face Hub. For that, you need to create an account in Hugging Face and set your `HF_TOKEN`:\n",
    "```\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "```\n",
    "Then, set `push_to_hub=True` in the `Seq2SeqTrainingArguments` or use the `push_to_hub()` method of the `Seq2SeqTrainer` class. Your token will be used to authenticate you with Hugging Face Hub, allowing you to push the model to your personal repository."
   ],
   "id": "e79825efe58a1e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# write your code here\n",
   "id": "1fcbedb39c9eb204"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transliteration\n",
    "In the final stage of this project, you are going to test your fine-tuned model to anglicize Polish names. You can use the test set you created earlier. You can use the `pipeline()` function from the `transformers` library to load the model and tokenizer:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"/content/model/\"  # the output dir or your model in HF hub if you pushed it there\n",
    "model = pipeline('text2text-generation', model=model_name, tokenizer=model_name)\n",
    "\n",
    "```\n",
    "Print the anglicized names and toponyms. Here's an example of what you'd expect to see (the output may vary):\n",
    "\n",
    "```text\n",
    "Original: Tobiasz, Anglicized: Tobias\n",
    "Original: Lubomir, Anglicized: Lubomir\n",
    "Original: Holandia, Anglicized: Holand\n",
    "Original: Łotwa, Anglicized: otwa\n",
    "Original: Celestyn, Anglicized: Celestin\n",
    "Original: Roland, Anglicized: Roland\n",
    "```\n"
   ],
   "id": "b0f29a944eb4ac9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# write your code here\n",
   "id": "92c908eae0c3a11a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As noted earlier, the model may not always produce the expected results. For example, it may anglicize *Łotwa* as *otwa* instead of *Latvia*. This is because the model was trained on a small dataset and may not have learned the correct mapping for all names and toponyms. However, it should still be able to produce reasonable results compared to the base T5 model.\n",
    "\n",
    "If you want to improve the model's performance, you can try fine-tuning it on a larger dataset or using a different model architecture. You can also experiment with different hyperparameters, such as the learning rate and batch size, to see if they affect the model's performance.\n",
    "\n",
    "Finally, you can also try using other models from the Hugging Face Hub that are fine-tuned for similar tasks. These models have been trained on larger datasets and will produce better results than the model you just fine-tuned. The fine-tuning process for these (and other) models is similar to what we did here, with a few adjustments to the dataset and hyperparameters.\n",
    "\n",
    "Try using the [sdadas/mt5-base-translator-pl-en](https://huggingface.co/sdadas/mt5-base-translator-pl-en) model, which is a fine-tuned version of the mT5 model for Polish to English translation tasks. You can load it in the same way as shown above and test it on your dataset."
   ],
   "id": "10ae86f545189131"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
